{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFi3diuZS5uQ",
    "outputId": "7141f113-6a4e-4bbe-e1db-40aa2dd08852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/raaif/.local/lib/python3.10/site-packages (4.38.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/raaif/.local/lib/python3.10/site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/raaif/.local/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/raaif/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/raaif/.local/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/raaif/.local/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/raaif/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/raaif/.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: filelock in /home/raaif/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/raaif/.local/lib/python3.10/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/raaif/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/raaif/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/raaif/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/raaif/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wmdhSqK9aj7V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30695/1701490621.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "\n",
    "file_path = \"train_data/dontpatronizeme_pcl.tsv\"\n",
    "train_filepath = \"dev_data/train_semeval_parids-labels.csv\"\n",
    "dev_filepath = \"dev_data/dev_semeval_parids-labels.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['id', 'paragraph-id', 'keyword', 'countrycode', \"paragraph\", \"label\"])\n",
    "df_filtered = df[df['paragraph'].notna()]\n",
    "\n",
    "# labels = [[0,1] if int(x)<=1 else [1,0] for x in df_filtered['label']]\n",
    "\n",
    "# labels = np.array(labels)\n",
    "\n",
    "# df_filtered['contains_pcl'] = df_filtered['label'].apply(lambda x: np.array([0,1]) if int(x) <= 1 else np.array([1,0])) # [True, False]\n",
    "\n",
    "train_df = pd.read_csv(train_filepath)\n",
    "dev_df = pd.read_csv(dev_filepath)\n",
    "\n",
    "train_data = df_filtered[df_filtered['id'].isin(train_df['par_id'])]\n",
    "dev_data = df_filtered[df_filtered['id'].isin(dev_df['par_id'])]\n",
    "\n",
    "train_data_shuffled = shuffle(train_data, random_state=42)\n",
    "dev_data_shuffled = shuffle(dev_data, random_state=42)\n",
    "\n",
    "X_train = train_data_shuffled['paragraph'].to_numpy()\n",
    "X_dev = dev_data_shuffled['paragraph'].to_numpy()\n",
    "\n",
    "y_train = np.array([[0,1] if int(x) <= 1 else [1,0] for x in train_data_shuffled['label']])\n",
    "y_dev = np.array([[0,1] if int(x) <= 1 else [1,0] for x in dev_data_shuffled['label']])\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the dataset into training (80%) and validation (20%) sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph-id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>countrycode</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4822</th>\n",
       "      <td>4823</td>\n",
       "      <td>@@8781228</td>\n",
       "      <td>disabled</td>\n",
       "      <td>gb</td>\n",
       "      <td>As well as lying about helping flood victims ,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8324</th>\n",
       "      <td>8325</td>\n",
       "      <td>@@14942580</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ng</td>\n",
       "      <td>Betty Abah is passionate about this initiative...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>2384</td>\n",
       "      <td>@@7600715</td>\n",
       "      <td>in-need</td>\n",
       "      <td>sg</td>\n",
       "      <td>\" He liked to help people so I thought this co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>4289</td>\n",
       "      <td>@@8869471</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ng</td>\n",
       "      <td>\" The airlines are relatively small , weak and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>5508</td>\n",
       "      <td>@@23720891</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ie</td>\n",
       "      <td>In general , people live inside their own bubb...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5846</th>\n",
       "      <td>5847</td>\n",
       "      <td>@@19919480</td>\n",
       "      <td>homeless</td>\n",
       "      <td>my</td>\n",
       "      <td>Last year , a record 85 homes were demolished ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5290</th>\n",
       "      <td>5291</td>\n",
       "      <td>@@21695353</td>\n",
       "      <td>homeless</td>\n",
       "      <td>pk</td>\n",
       "      <td>\" As a country , we can look for the missed op...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>5492</td>\n",
       "      <td>@@14069020</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>hk</td>\n",
       "      <td>Any opening in which the speakers can revert t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>@@24188457</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ke</td>\n",
       "      <td>Dennis insisted that his initiative was not in...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7421</th>\n",
       "      <td>7422</td>\n",
       "      <td>@@1431279</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ca</td>\n",
       "      <td>Developing something like this for commercial ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8375 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id paragraph-id     keyword countrycode  \\\n",
       "4822  4823    @@8781228    disabled          gb   \n",
       "8324  8325   @@14942580  vulnerable          ng   \n",
       "2383  2384    @@7600715     in-need          sg   \n",
       "4288  4289    @@8869471  vulnerable          ng   \n",
       "5507  5508   @@23720891     refugee          ie   \n",
       "...    ...          ...         ...         ...   \n",
       "5846  5847   @@19919480    homeless          my   \n",
       "5290  5291   @@21695353    homeless          pk   \n",
       "5491  5492   @@14069020   immigrant          hk   \n",
       "879    880   @@24188457     in-need          ke   \n",
       "7421  7422    @@1431279    hopeless          ca   \n",
       "\n",
       "                                              paragraph  label  \n",
       "4822  As well as lying about helping flood victims ,...      0  \n",
       "8324  Betty Abah is passionate about this initiative...      4  \n",
       "2383  \" He liked to help people so I thought this co...      1  \n",
       "4288  \" The airlines are relatively small , weak and...      0  \n",
       "5507  In general , people live inside their own bubb...      3  \n",
       "...                                                 ...    ...  \n",
       "5846  Last year , a record 85 homes were demolished ...      0  \n",
       "5290  \" As a country , we can look for the missed op...      0  \n",
       "5491  Any opening in which the speakers can revert t...      0  \n",
       "879   Dennis insisted that his initiative was not in...      3  \n",
       "7421  Developing something like this for commercial ...      0  \n",
       "\n",
       "[8375 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph-id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>countrycode</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10056</th>\n",
       "      <td>10057</td>\n",
       "      <td>@@4197415</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>ca</td>\n",
       "      <td>Darte acknowledged cutting back to the Windsor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9650</th>\n",
       "      <td>9651</td>\n",
       "      <td>@@25216962</td>\n",
       "      <td>migrant</td>\n",
       "      <td>bd</td>\n",
       "      <td>UNITED States President Donald Trump has defen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9119</th>\n",
       "      <td>9120</td>\n",
       "      <td>@@22467955</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ca</td>\n",
       "      <td>Saraswat said most immigrants have unique livi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8504</th>\n",
       "      <td>8505</td>\n",
       "      <td>@@10179731</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>pk</td>\n",
       "      <td>He said some elements were bent upon spreading...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>1283</td>\n",
       "      <td>@@3208839</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ph</td>\n",
       "      <td>\" Stateless \" is the story of a forgotten grou...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>9978</td>\n",
       "      <td>@@13589752</td>\n",
       "      <td>homeless</td>\n",
       "      <td>in</td>\n",
       "      <td>One response to marital infidelity is divorce ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9384</th>\n",
       "      <td>9385</td>\n",
       "      <td>@@1955909</td>\n",
       "      <td>homeless</td>\n",
       "      <td>tz</td>\n",
       "      <td>Various other areas have been experiencing exc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9423</th>\n",
       "      <td>9424</td>\n",
       "      <td>@@18374692</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ca</td>\n",
       "      <td>Chris Selley : Maybe liquor retail in Ontario ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9594</th>\n",
       "      <td>9595</td>\n",
       "      <td>@@1065878</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>us</td>\n",
       "      <td>Robin Wauters is the European Editor of The Ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9136</th>\n",
       "      <td>9137</td>\n",
       "      <td>@@52188</td>\n",
       "      <td>disabled</td>\n",
       "      <td>ie</td>\n",
       "      <td>\" It ranges from members of the Oireachtas , t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2093 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id paragraph-id        keyword countrycode  \\\n",
       "10056  10057    @@4197415  poor-families          ca   \n",
       "9650    9651   @@25216962        migrant          bd   \n",
       "9119    9120   @@22467955      immigrant          ca   \n",
       "8504    8505   @@10179731       hopeless          pk   \n",
       "1282    1283    @@3208839        refugee          ph   \n",
       "...      ...          ...            ...         ...   \n",
       "9977    9978   @@13589752       homeless          in   \n",
       "9384    9385    @@1955909       homeless          tz   \n",
       "9423    9424   @@18374692       hopeless          ca   \n",
       "9594    9595    @@1065878       hopeless          us   \n",
       "9136    9137      @@52188       disabled          ie   \n",
       "\n",
       "                                               paragraph  label  \n",
       "10056  Darte acknowledged cutting back to the Windsor...      0  \n",
       "9650   UNITED States President Donald Trump has defen...      0  \n",
       "9119   Saraswat said most immigrants have unique livi...      0  \n",
       "8504   He said some elements were bent upon spreading...      0  \n",
       "1282   \" Stateless \" is the story of a forgotten grou...      4  \n",
       "...                                                  ...    ...  \n",
       "9977   One response to marital infidelity is divorce ...      0  \n",
       "9384   Various other areas have been experiencing exc...      0  \n",
       "9423   Chris Selley : Maybe liquor retail in Ontario ...      0  \n",
       "9594   Robin Wauters is the European Editor of The Ne...      0  \n",
       "9136   \" It ranges from members of the Oireachtas , t...      0  \n",
       "\n",
       "[2093 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cQK6U3yuTfo",
    "outputId": "9c0848cc-00e4-4462-d960-90b1ef62cb8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        ...,\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor(y_train)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346,
     "referenced_widgets": [
      "c89ded56995f48b4a074422989f46ce2",
      "579e015f20024190861e36d8bae9bdcb",
      "e7899a82b3134f84ba1f3e0c45ddb1f5",
      "ccc160fb75d54b7cb727cf5b25d847d6",
      "a3d741e6a0a94eaa90a5b4c4042ca349",
      "cf18fec3a52c46d0869097f52a0c002c",
      "e7fb6083b8464287a536d8a4032d3cd8",
      "86e14566cc3f4d59b949c4e1219a8206",
      "33716718d9cb426d93dc03c5c00bdd2f",
      "0538292773c540f39b5af23e0615b063",
      "49ab33ebbc8e4401a34d6b616846eada",
      "545a8c5b3288488aa43ed169a7124539",
      "5f485543b68e44da96dd5d6a8b087d47",
      "19632de672904bb19d53996c5f61a51f",
      "78442e12e51a4c6288907cb9342347de",
      "909c888073b3492b9c37702b274b9eb4",
      "c7d514670f0547d3a3327b43b04e8e09",
      "c33fff6421db4235a17f70b22a37abd8",
      "02b9a6a75490438da5548ce15a0e3d8a",
      "ab1eae8c2a3347cc8494766fba11118c",
      "f64122c8ce5248859dfdf340c34f917f",
      "b9902f7d9a0d4cdbbca93d75ff1890c2",
      "d911e2cb6c964314a58bdfee2562276d",
      "418e255647c04016ba56a24bc35e5217",
      "1a42d73d30bd4f6880c46526fc3ede83",
      "8f581039bd7848d1b892303f938ebfad",
      "17f7e00bc9e745ea9561b07e7e926acf",
      "2c140a9be8b14ffd8f6ee533b02a7ae9",
      "3a9482003ee541b291216dc4b4cc6dc8",
      "7dcb99d856b94b068f4efcf783276c4e",
      "812d8e4ff4384d64a55ce81f98588bb1",
      "f6a1ee9f3f57440dbb051f12ead5f662",
      "8b02f21d8bbc4c669c17ec2299cc8255",
      "694a40dbace64ca6b608b3ecb85042f5",
      "59076efdcdac4ef484d8df7a2171cc22",
      "737dd8afd60a4284ad9310d09ea37809",
      "04d02facb2d6405da3d853450339ebb2",
      "912bd3c591f74305a49c1a4247155c6d",
      "3db025d6b1284221a8d5e6aba5cb7ffd",
      "34ecc7c661cc4fa2b3228137458fd9ba",
      "73870ae9ed204cf8bd468921ac353abc",
      "c13e9516153e44db8395be037a3d54d8",
      "ae0b3835321b46e0b39fedc3fcab3cce",
      "4b095ae8c9504e2ab2285802ed35ed91"
     ]
    },
    "id": "DAw5gcF_bHpV",
    "outputId": "da8dc549-46da-4a0e-9e80-69c296350d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOriginal:  Our friends won\\'t buy this analysis, let alone the next one we propose.\\nTokenized:  [\\'our\\', \\'friends\\', \\'won\\', \"\\'\", \\'t\\', \\'buy\\', \\'this\\', \\'analysis\\', \\',\\', \\'let\\', \\'alone\\', \\'the\\', \\'next\\', \\'one\\', \\'we\\', \\'propose\\', \\'.\\']\\nToken IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "'''\n",
    "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
    "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
    "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVUX7jVibLa7",
    "outputId": "375959de-ed1d-4021-a727-95594060b3f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/raaif/.local/lib/python3.10/site-packages (0.16.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/raaif/.local/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/raaif/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/raaif/.local/lib/python3.10/site-packages (from wandb) (5.9.6)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/raaif/.local/lib/python3.10/site-packages (from wandb) (3.1.42)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/raaif/.local/lib/python3.10/site-packages (from wandb) (1.40.6)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/raaif/.local/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: setproctitle in /home/raaif/.local/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/raaif/.local/lib/python3.10/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/lib/python3/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (59.6.0)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/raaif/.local/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/raaif/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/raaif/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2020.6.20)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/raaif/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "Bc7RDwlTbokA",
    "outputId": "49fc6b4d-87ff-49ed-b99e-3c653c90c028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8f10ukua\n",
      "Sweep URL: https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random', #grid, random\n",
    "    'metric': {\n",
    "      'name': 'val_f1_score',\n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "\n",
    "        'learning_rate': {\n",
    "            'values': [5e-5, 3e-5, 2e-5]\n",
    "        },\n",
    "        # 'batch_size': {\n",
    "        #     'values': [8, 16]\n",
    "        # },\n",
    "        'epochs': {\n",
    "            'values': [6, 8, 10]\n",
    "        },\n",
    "        'warmup_percentage': {\n",
    "            'values': [0.05, 0.1]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "sweep_defaults = {\n",
    "        'learning_rate': 5e-5,\n",
    "\n",
    "        # 'batch_size': 16,\n",
    "\n",
    "        'epochs': 4,\n",
    "\n",
    "        'warmup_percentage': 0.1\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-ZOBiV3bq8A",
    "outputId": "11e8e609-1e16-46e6-f895-9207a41b21f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  As well as lying about helping flood victims , he also made false claims about using the company 's cash to help disabled children .\n",
      "Token IDs: tensor([ 101, 2004, 2092, 2004, 4688, 2055, 5094, 7186, 5694, 1010, 2002, 2036,\n",
      "        2081, 6270, 4447, 2055, 2478, 1996, 2194, 1005, 1055, 5356, 2000, 2393,\n",
      "        9776, 2336, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Attention Mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "labels tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in X_train:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 300,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(y_train)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', X_train[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print('Attention Mask:', attention_masks)\n",
    "print('labels', labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IYY_M490cR8C",
    "outputId": "f7975e8a-0705-4114-ffa4-ce1579dc391e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,537 training samples\n",
      "  838 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vFe52yxIeC_6"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import wandb\n",
    "# WANDB PARAMETER\n",
    "def ret_dataloader():\n",
    "    # batch_size = wandb.config.batch_size\n",
    "    batch_size = 16\n",
    "    print('batch_size = ', batch_size)\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                batch_size = batch_size # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )\n",
    "    return train_dataloader,validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nQDcl3OEeFTK"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "def ret_model():\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels = 2,\n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9csyoIEPeHsY"
   },
   "outputs": [],
   "source": [
    "def ret_optim(model):\n",
    "    print('Learning_rate = ',wandb.config.learning_rate )\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = wandb.config.learning_rate,\n",
    "                      eps = 1e-8\n",
    "                    )\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "d5sR3_lpeIwG"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def ret_scheduler(train_dataloader,optimizer):\n",
    "    epochs = wandb.config.epochs\n",
    "    print('epochs = ', epochs)\n",
    "    # Total number of training steps is [number of batches] x [number of epochs].\n",
    "    # (Note that this is not the same as the number of training samples).\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    wp = wandb.config.warmup_percentage\n",
    "    print('warmup percentage', wp)\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps = wp * total_steps,\n",
    "                                                num_training_steps = total_steps)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "U0Gy3Q9meJyU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "# def flat_accuracy(preds, labels):\n",
    "#     pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "#     labels_flat = labels.flatten()\n",
    "#     return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XCyAAMhl8CW",
    "outputId": "9e7b72ce-2b65-40dd-dd04-e47451dfadfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchmetrics in /home/raaif/.local/lib/python3.10/site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy>1.20.0 in /home/raaif/.local/lib/python3.10/site-packages (from torchmetrics) (1.26.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/raaif/.local/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: packaging>17.1 in /home/raaif/.local/lib/python3.10/site-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/raaif/.local/lib/python3.10/site-packages (from torchmetrics) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions in /home/raaif/.local/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (59.6.0)\n",
      "Requirement already satisfied: jinja2 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2.18.1)\n",
      "Requirement already satisfied: fsspec in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
      "Requirement already satisfied: filelock in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/raaif/.local/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/raaif/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.3.52)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/raaif/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Vw8DmRemeMN3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from torchmetrics import Precision, F1Score, Accuracy\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Initialize metrics\n",
    "# Assuming you're working on a binary classification problem\n",
    "task = 'binary' # Use 'multiclass' for multi-class classification\n",
    "\n",
    "# Initialize metrics with the task parameter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# precision = Precision(num_classes=2, average='macro', task=task).to(device)\n",
    "# f1_score = F1Score(num_classes=2, average='macro', task=task).to(device)\n",
    "accuracy = Accuracy(num_classes=2, average='macro', task=task).to(device)\n",
    "\n",
    "    # This training code is based on the `run_glue.py` script here:\n",
    "    # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "def train():\n",
    "\n",
    "    wandb.init()\n",
    "    \n",
    "    # print(device)\n",
    "    model = ret_model()\n",
    "    model.to(device)\n",
    "    #wandb.init(config=sweep_defaults)\n",
    "    train_dataloader, validation_dataloader = ret_dataloader()\n",
    "    optimizer = ret_optim(model)\n",
    "    scheduler = ret_scheduler(train_dataloader,optimizer)\n",
    "\n",
    "    #print(\"config \",wandb.config.learning_rate, \"\\n\",wandb.config)\n",
    "    seed_val = 42\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    #torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss,\n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    epochs = wandb.config.epochs\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "        # precision.reset()\n",
    "        # f1_score.reset()\n",
    "\n",
    "        # print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        # print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        # total_train_precision = 0\n",
    "        # total_train_f1 = 0\n",
    "\n",
    "        # all_train_logits = []\n",
    "        # all_train_labels = []\n",
    "\n",
    "        total_TP = 0\n",
    "        total_FP = 0\n",
    "        total_FN = 0\n",
    "        total_TN = 0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to\n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids\n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because\n",
    "            # accumulating the gradients is \"convenient while training RNNs\".\n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here:\n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            # print(b_labels)\n",
    "            # print(b_input_ids)\n",
    "            # print(b_input_ids.shape)\n",
    "            outputs = model(b_input_ids,\n",
    "                                attention_mask=b_input_mask)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            # print(logits)\n",
    "            \n",
    "            loss = F.cross_entropy(logits.float(), b_labels.float())\n",
    "\n",
    "            # all_train_logits.append(logits)\n",
    "            # all_train_labels.append(b_labels)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1).cpu()\n",
    "            b_labels_vals = np.array([b[0].cpu().numpy() for b in b_labels])\n",
    "\n",
    "            # print(predictions.shape)\n",
    "            # print(b_labels_vals.shape)\n",
    "\n",
    "            TP = ((predictions == 1) & (b_labels_vals == 1)).sum().item()\n",
    "            FP = ((predictions == 1) & (b_labels_vals == 0)).sum().item()\n",
    "            FN = ((predictions == 0) & (b_labels_vals == 1)).sum().item()\n",
    "            TN = ((predictions == 0) & (b_labels_vals == 0)).sum().item()\n",
    "\n",
    "            total_TP += TP\n",
    "            total_FP += FP\n",
    "            total_FN += FN\n",
    "            total_TN += TN\n",
    "            \n",
    "            # Update precision and F1 score\n",
    "            # print(\"Logits: \",logits, \"Labels:\", b_labels)\n",
    "            \n",
    "            # precision.update(logits, b_labels)\n",
    "            # f1_score.update(logits, b_labels)\n",
    "\n",
    "\n",
    "            # wandb.log(\n",
    "            #     {'train_batch_loss':loss.item()})\n",
    "                # 'train_batch_precision': precision.compute().item(),\n",
    "                # 'train_batch_f1_score': f1_score.compute().item()})\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value\n",
    "            # from the tensor.\n",
    "            total_train_loss += loss.item()\n",
    "            # total_train_precision += precision.compute().item()\n",
    "            # total_train_f1 += f1_score.compute().item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # train_logits = torch.cat(all_train_logits, dim=0)\n",
    "        # train_labels = torch.cat(all_train_labels, dim=0)\n",
    "\n",
    "        # precision.update(train_logits, train_labels)\n",
    "        # f1_score.update(train_logits, train_labels)\n",
    "\n",
    "        precision = total_TP / (total_TP + total_FP) if total_TP + total_FP > 0 else 0\n",
    "        recall = total_TP / (total_TP + total_FN) if total_TP + total_FN > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        # avg_train_precision = total_train_precision / len(train_dataloader)\n",
    "        # avg_train_f1 = total_train_f1 / len(train_dataloader)\n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        # train_precision = precision.compute().item()\n",
    "        # train_f1 = f1_score.compute().item()\n",
    "\n",
    "        wandb.log({'avg_train_loss': avg_train_loss})\n",
    "        wandb.log({'train_precision': precision})\n",
    "        wandb.log({'train_f1': f1_score})\n",
    "\n",
    "        # precision.reset()\n",
    "        # f1_score.reset()\n",
    "        accuracy.reset()\n",
    "\n",
    "        # print(\"\")\n",
    "        # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        # print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        # print(\"\")\n",
    "        # print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # total_eval_precision = 0\n",
    "        # total_eval_f1 = 0\n",
    "\n",
    "        # all_val_logits = []\n",
    "        # all_val_labels = []\n",
    "\n",
    "        total_TP = 0\n",
    "        total_FP = 0\n",
    "        total_FN = 0\n",
    "        total_TN = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader.\n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids\n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels\n",
    "            b_input_ids = batch[0].cuda()\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which\n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here:\n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                # print(\"HEREEEEEE\")\n",
    "                # print(b_input_ids.shape)\n",
    "                \n",
    "                outputs = model(b_input_ids,\n",
    "                                      attention_mask=b_input_mask)\n",
    "            \n",
    "            logits = outputs.logits\n",
    "\n",
    "            # print(logits.shape)\n",
    "            # print(b_labels.shape)\n",
    "            \n",
    "            loss = F.cross_entropy(logits.float(), b_labels.float())\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1).cpu()\n",
    "            b_labels_vals = np.array([b[0].cpu().numpy() for b in b_labels])\n",
    "\n",
    "            # print(predictions.shape)\n",
    "            # print(b_labels_vals.shape)\n",
    "\n",
    "            TP = ((predictions == 1) & (b_labels_vals == 1)).sum().item()\n",
    "            FP = ((predictions == 1) & (b_labels_vals == 0)).sum().item()\n",
    "            FN = ((predictions == 0) & (b_labels_vals == 1)).sum().item()\n",
    "            TN = ((predictions == 0) & (b_labels_vals == 0)).sum().item()\n",
    "\n",
    "            total_TP += TP\n",
    "            total_FP += FP\n",
    "            total_FN += FN\n",
    "            total_TN += TN\n",
    "\n",
    "            # all_val_logits.append(logits)\n",
    "            # all_val_labels.append(b_labels)\n",
    "\n",
    "            # print(\"got validation loss without throwing a fit\")\n",
    "\n",
    "            # precision.update(logits, b_labels)\n",
    "            # f1_score.update(logits, b_labels)\n",
    "            accuracy.update(logits, b_labels)\n",
    "\n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "            # total_eval_precision += precision.compute().item()\n",
    "            # total_eval_f1 += f1_score.compute().item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_eval_accuracy += accuracy.compute().item()\n",
    "\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        # print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        # avg_val_precision = total_eval_precision / len(validation_dataloader)\n",
    "        # avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "\n",
    "        # val_logits = torch.cat(all_val_logits, dim=0)\n",
    "        # val_labels = torch.cat(all_val_labels, dim=0)\n",
    "\n",
    "        # accuracy.update(val_logits, val_labels)\n",
    "        # precision.update(val_logits, val_labels)\n",
    "        # f1_score.update(val_logits, val_labels)\n",
    "\n",
    "        # val_accuracy = accuracy.compute().item()\n",
    "        # val_precision = precision.compute().item()\n",
    "        # val_f1 = f1_score.compute().item()\n",
    "\n",
    "        precision = total_TP / (total_TP + total_FP) if total_TP + total_FP > 0 else 0\n",
    "        recall = total_TP / (total_TP + total_FN) if total_TP + total_FN > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        wandb.log({'val_accuracy': avg_val_accuracy, \n",
    "                   'avg_val_loss': avg_val_loss, \n",
    "                   'val_precision': precision, \n",
    "                   'val_f1': f1_score})\n",
    "        # print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        # print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Valid. F1': f1_score,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Call the garbage collector\n",
    "    gc.collect()\n",
    "    \n",
    "    # Ensure CUDA is aware of the freed memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "222d862bfe6b4c86a4407447526ee2b6",
      "483f44483ea24b86af2f619c5c0f065e",
      "fb8ed742afeb468a86e96c733957b08f",
      "7bcac0265f33462ba8454ea8f38b0ca5",
      "5432be1e78f243a0b08c423cc845f31a",
      "e65fc40dcef74ed9be651ec3c82fa933",
      "496d7105b9944fe3bd6fec38082656a7",
      "ebb15b86889a49b6965b2a697dc5246b"
     ]
    },
    "id": "-_BGhk3SeOsZ",
    "outputId": "48890124-a5a7-47d5-d9b6-718c6be92f42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3cv8kbhy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_percentage: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m1hraaif\u001b[0m (\u001b[33mraaif\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/raaif/Desktop/Uni/y4/70016 - NLP/NLP-CW/wandb/run-20240305_023724-3cv8kbhy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raaif/uncategorized/runs/3cv8kbhy' target=\"_blank\">magic-sweep-1</a></strong> to <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">https://wandb.ai/raaif/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raaif/uncategorized/runs/3cv8kbhy' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/3cv8kbhy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  3e-05\n",
      "epochs =  10\n",
      "warmup percentage 0.05\n",
      "======== Epoch 1 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:54.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:05.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:16.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:27.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:38.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:49.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:00.\n",
      "======== Epoch 2 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 3 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 4 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 5 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 6 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 7 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 8 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 9 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 10 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "Training complete!\n",
      "Total training took 0:22:28 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▆▃▂▁▁▁▁▁▁</td></tr><tr><td>avg_val_loss</td><td>▁▁▃▃▅▇▇███</td></tr><tr><td>train_f1</td><td>█▅▃▂▁▁▁▁▁▁</td></tr><tr><td>train_precision</td><td>█▅▃▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▃▆▂▃▁▄▆▇██</td></tr><tr><td>val_f1</td><td>█▆▆▂▁▆▄▄▅▃</td></tr><tr><td>val_precision</td><td>█▆▆▁▁▆▄▄▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.0008</td></tr><tr><td>avg_val_loss</td><td>0.57585</td></tr><tr><td>train_f1</td><td>0.00027</td></tr><tr><td>train_precision</td><td>0.00015</td></tr><tr><td>val_accuracy</td><td>0.93219</td></tr><tr><td>val_f1</td><td>0.08</td></tr><tr><td>val_precision</td><td>0.04376</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-sweep-1</strong> at: <a href='https://wandb.ai/raaif/uncategorized/runs/3cv8kbhy' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/3cv8kbhy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240305_023724-3cv8kbhy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qvwag9xn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_percentage: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/raaif/Desktop/Uni/y4/70016 - NLP/NLP-CW/wandb/run-20240305_030009-qvwag9xn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raaif/uncategorized/runs/qvwag9xn' target=\"_blank\">crimson-sweep-2</a></strong> to <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">https://wandb.ai/raaif/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raaif/uncategorized/runs/qvwag9xn' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/qvwag9xn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  5e-05\n",
      "epochs =  8\n",
      "warmup percentage 0.1\n",
      "======== Epoch 1 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 2 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 3 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 4 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 5 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 6 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 7 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 8 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "Training complete!\n",
      "Total training took 0:18:00 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▆▃▂▁▁▁▁</td></tr><tr><td>avg_val_loss</td><td>▂▁▃▅▅▆██</td></tr><tr><td>train_f1</td><td>█▅▃▁▁▁▁▁</td></tr><tr><td>train_precision</td><td>█▅▃▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄█▇▇█▆▅</td></tr><tr><td>val_f1</td><td>█▃▁▂▁▁▅▄</td></tr><tr><td>val_precision</td><td>█▃▁▂▁▁▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>3e-05</td></tr><tr><td>avg_val_loss</td><td>0.54956</td></tr><tr><td>train_f1</td><td>0</td></tr><tr><td>train_precision</td><td>0.0</td></tr><tr><td>val_accuracy</td><td>0.92727</td></tr><tr><td>val_f1</td><td>0.09491</td></tr><tr><td>val_precision</td><td>0.05183</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-sweep-2</strong> at: <a href='https://wandb.ai/raaif/uncategorized/runs/qvwag9xn' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/qvwag9xn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240305_030009-qvwag9xn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lxzpdca9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_percentage: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/raaif/Desktop/Uni/y4/70016 - NLP/NLP-CW/wandb/run-20240305_031818-lxzpdca9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raaif/uncategorized/runs/lxzpdca9' target=\"_blank\">generous-sweep-3</a></strong> to <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">https://wandb.ai/raaif/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raaif/uncategorized/runs/lxzpdca9' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/lxzpdca9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  3e-05\n",
      "epochs =  6\n",
      "warmup percentage 0.1\n",
      "======== Epoch 1 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 2 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 3 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 4 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 5 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 6 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "Training complete!\n",
      "Total training took 0:13:30 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.012 MB uploaded\\r'), FloatProgress(value=0.22975843890156583, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▆▃▂▁▁</td></tr><tr><td>avg_val_loss</td><td>▁▁▃▅██</td></tr><tr><td>train_f1</td><td>█▅▃▂▁▁</td></tr><tr><td>train_precision</td><td>█▅▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▆▆▁█▄▅</td></tr><tr><td>val_f1</td><td>█▇▄▄▁▄</td></tr><tr><td>val_precision</td><td>█▇▄▄▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.00188</td></tr><tr><td>avg_val_loss</td><td>0.46142</td></tr><tr><td>train_f1</td><td>0.0008</td></tr><tr><td>train_precision</td><td>0.00044</td></tr><tr><td>val_accuracy</td><td>0.92471</td></tr><tr><td>val_f1</td><td>0.08471</td></tr><tr><td>val_precision</td><td>0.04633</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">generous-sweep-3</strong> at: <a href='https://wandb.ai/raaif/uncategorized/runs/lxzpdca9' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/lxzpdca9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240305_031818-lxzpdca9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lgy6255z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_percentage: 0.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/raaif/Desktop/Uni/y4/70016 - NLP/NLP-CW/wandb/run-20240305_033156-lgy6255z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raaif/uncategorized/runs/lgy6255z' target=\"_blank\">elated-sweep-4</a></strong> to <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">https://wandb.ai/raaif/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raaif/uncategorized/runs/lgy6255z' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/lgy6255z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  5e-05\n",
      "epochs =  8\n",
      "warmup percentage 0.05\n",
      "======== Epoch 1 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 2 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 3 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 4 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 5 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 6 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 7 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 8 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "Training complete!\n",
      "Total training took 0:18:00 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▆▅▃▂▁▁▁</td></tr><tr><td>avg_val_loss</td><td>▂▁▂▅▄▅▇█</td></tr><tr><td>train_f1</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>train_precision</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▂▁▂▆█▇▆▇</td></tr><tr><td>val_f1</td><td>█▃▁▁▂▃▃▃</td></tr><tr><td>val_precision</td><td>█▃▁▁▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.00739</td></tr><tr><td>avg_val_loss</td><td>0.48118</td></tr><tr><td>train_f1</td><td>0.00265</td></tr><tr><td>train_precision</td><td>0.00146</td></tr><tr><td>val_accuracy</td><td>0.93958</td></tr><tr><td>val_f1</td><td>0.09513</td></tr><tr><td>val_precision</td><td>0.05196</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-sweep-4</strong> at: <a href='https://wandb.ai/raaif/uncategorized/runs/lgy6255z' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/lgy6255z</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240305_033156-lgy6255z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ftw7cy1f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_percentage: 0.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/raaif/Desktop/Uni/y4/70016 - NLP/NLP-CW/wandb/run-20240305_035008-ftw7cy1f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raaif/uncategorized/runs/ftw7cy1f' target=\"_blank\">lilac-sweep-5</a></strong> to <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">https://wandb.ai/raaif/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raaif/uncategorized/runs/ftw7cy1f' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/ftw7cy1f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  5e-05\n",
      "epochs =  6\n",
      "warmup percentage 0.05\n",
      "======== Epoch 1 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 2 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 3 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 4 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 5 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 6 / 6 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "Training complete!\n",
      "Total training took 0:13:30 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.008 MB uploaded\\r'), FloatProgress(value=0.36466633466135456, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▆▄▂▁▁</td></tr><tr><td>avg_val_loss</td><td>▃▁▃▅▆█</td></tr><tr><td>train_f1</td><td>█▆▃▂▁▁</td></tr><tr><td>train_precision</td><td>█▆▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▇▇███</td></tr><tr><td>val_f1</td><td>▁█▁▆▆▇</td></tr><tr><td>val_precision</td><td>▁█▁▆▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.01068</td></tr><tr><td>avg_val_loss</td><td>0.45879</td></tr><tr><td>train_f1</td><td>0.00344</td></tr><tr><td>train_precision</td><td>0.0019</td></tr><tr><td>val_accuracy</td><td>0.93419</td></tr><tr><td>val_f1</td><td>0.10575</td></tr><tr><td>val_precision</td><td>0.05772</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-sweep-5</strong> at: <a href='https://wandb.ai/raaif/uncategorized/runs/ftw7cy1f' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/ftw7cy1f</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240305_035008-ftw7cy1f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 22jx8yll with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_percentage: 0.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/raaif/Desktop/Uni/y4/70016 - NLP/NLP-CW/wandb/run-20240305_040346-22jx8yll</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raaif/uncategorized/runs/22jx8yll' target=\"_blank\">winter-sweep-6</a></strong> to <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">https://wandb.ai/raaif/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raaif/uncategorized/runs/22jx8yll' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/22jx8yll</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  5e-05\n",
      "epochs =  10\n",
      "warmup percentage 0.05\n",
      "======== Epoch 1 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 2 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 3 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 4 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 5 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 6 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 7 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 8 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 9 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 10 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "Training complete!\n",
      "Total training took 0:22:30 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▆▃▂▁▁▁▁▁▁</td></tr><tr><td>avg_val_loss</td><td>▂▁▄▄▅▆▇███</td></tr><tr><td>train_f1</td><td>█▅▃▂▁▁▁▁▁▁</td></tr><tr><td>train_precision</td><td>█▅▃▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▅█▁▆▇▅▇▅▇▅</td></tr><tr><td>val_f1</td><td>█▅▁▃▅▂▄▅▅▄</td></tr><tr><td>val_precision</td><td>█▅▁▃▅▂▄▅▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>2e-05</td></tr><tr><td>avg_val_loss</td><td>0.67723</td></tr><tr><td>train_f1</td><td>0</td></tr><tr><td>train_precision</td><td>0.0</td></tr><tr><td>val_accuracy</td><td>0.92204</td></tr><tr><td>val_f1</td><td>0.10209</td></tr><tr><td>val_precision</td><td>0.05577</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-6</strong> at: <a href='https://wandb.ai/raaif/uncategorized/runs/22jx8yll' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/22jx8yll</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240305_040346-22jx8yll/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2uco66si with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_percentage: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/raaif/Desktop/Uni/y4/70016 - NLP/NLP-CW/wandb/run-20240305_042627-2uco66si</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raaif/uncategorized/runs/2uco66si' target=\"_blank\">vocal-sweep-7</a></strong> to <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">https://wandb.ai/raaif/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raaif/uncategorized/runs/2uco66si' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/2uco66si</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  3e-05\n",
      "epochs =  8\n",
      "warmup percentage 0.1\n",
      "======== Epoch 1 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 2 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 3 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 4 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 5 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 6 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 7 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "======== Epoch 8 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:01.\n",
      "Training complete!\n",
      "Total training took 0:17:59 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▅▃▂▁▁▁▁</td></tr><tr><td>avg_val_loss</td><td>▁▁▄▆▆▇██</td></tr><tr><td>train_f1</td><td>█▆▃▁▁▁▁▁</td></tr><tr><td>train_precision</td><td>█▆▃▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▇▂▆██▇█</td></tr><tr><td>val_f1</td><td>▇█▁▂▁▃▃▂</td></tr><tr><td>val_precision</td><td>▇█▁▂▁▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.00048</td></tr><tr><td>avg_val_loss</td><td>0.4947</td></tr><tr><td>train_f1</td><td>0.00053</td></tr><tr><td>train_precision</td><td>0.00029</td></tr><tr><td>val_accuracy</td><td>0.92978</td></tr><tr><td>val_f1</td><td>0.08431</td></tr><tr><td>val_precision</td><td>0.04609</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-7</strong> at: <a href='https://wandb.ai/raaif/uncategorized/runs/2uco66si' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/2uco66si</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240305_042627-2uco66si/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 25rgh51f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_percentage: 0.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/raaif/Desktop/Uni/y4/70016 - NLP/NLP-CW/wandb/run-20240305_044435-25rgh51f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raaif/uncategorized/runs/25rgh51f' target=\"_blank\">amber-sweep-8</a></strong> to <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">https://wandb.ai/raaif/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raaif/uncategorized/runs/25rgh51f' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/25rgh51f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  5e-05\n",
      "epochs =  10\n",
      "warmup percentage 0.05\n",
      "======== Epoch 1 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:39.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:50.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 2 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 3 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 4 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 5 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 6 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:28.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 7 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 8 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 9 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 10 / 10 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "Training complete!\n",
      "Total training took 0:22:32 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▆▃▂▁▁▁▁▁▁</td></tr><tr><td>avg_val_loss</td><td>▁▁▃▄▆▇▇▇██</td></tr><tr><td>train_f1</td><td>█▅▃▂▁▁▁▁▁▁</td></tr><tr><td>train_precision</td><td>█▅▃▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▃▇▁▇▇▇▅█▇▇</td></tr><tr><td>val_f1</td><td>▆█▁▄▃▅▃▄▄▄</td></tr><tr><td>val_precision</td><td>▆█▁▄▃▅▃▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>1e-05</td></tr><tr><td>avg_val_loss</td><td>0.66837</td></tr><tr><td>train_f1</td><td>0</td></tr><tr><td>train_precision</td><td>0.0</td></tr><tr><td>val_accuracy</td><td>0.93146</td></tr><tr><td>val_f1</td><td>0.09919</td></tr><tr><td>val_precision</td><td>0.05416</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-8</strong> at: <a href='https://wandb.ai/raaif/uncategorized/runs/25rgh51f' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/25rgh51f</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240305_044435-25rgh51f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5cbuskb0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_percentage: 0.05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/raaif/Desktop/Uni/y4/70016 - NLP/NLP-CW/wandb/run-20240305_050714-5cbuskb0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raaif/uncategorized/runs/5cbuskb0' target=\"_blank\">blooming-sweep-9</a></strong> to <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">https://wandb.ai/raaif/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raaif/uncategorized/runs/5cbuskb0' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/5cbuskb0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  2e-05\n",
      "epochs =  8\n",
      "warmup percentage 0.05\n",
      "======== Epoch 1 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 2 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:07.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 3 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 4 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 5 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:17.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 6 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 7 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 8 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "Training complete!\n",
      "Total training took 0:18:03 (h:mm:ss)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>█▅▄▂▁▁▁▁</td></tr><tr><td>avg_val_loss</td><td>▂▁▂▄▆▇██</td></tr><tr><td>train_f1</td><td>█▆▃▂▁▁▁▁</td></tr><tr><td>train_precision</td><td>█▆▃▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▃▅▃▅█▅</td></tr><tr><td>val_f1</td><td>█▃▁▁▂▁▃▃</td></tr><tr><td>val_precision</td><td>█▃▁▁▂▁▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.00047</td></tr><tr><td>avg_val_loss</td><td>0.49447</td></tr><tr><td>train_f1</td><td>0.00027</td></tr><tr><td>train_precision</td><td>0.00015</td></tr><tr><td>val_accuracy</td><td>0.92498</td></tr><tr><td>val_f1</td><td>0.0908</td></tr><tr><td>val_precision</td><td>0.04962</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-sweep-9</strong> at: <a href='https://wandb.ai/raaif/uncategorized/runs/5cbuskb0' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/5cbuskb0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240305_050714-5cbuskb0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8o3e4ti4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_percentage: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/raaif/Desktop/Uni/y4/70016 - NLP/NLP-CW/wandb/run-20240305_052527-8o3e4ti4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raaif/uncategorized/runs/8o3e4ti4' target=\"_blank\">fine-sweep-10</a></strong> to <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raaif/uncategorized' target=\"_blank\">https://wandb.ai/raaif/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua' target=\"_blank\">https://wandb.ai/raaif/uncategorized/sweeps/8f10ukua</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raaif/uncategorized/runs/8o3e4ti4' target=\"_blank\">https://wandb.ai/raaif/uncategorized/runs/8o3e4ti4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raaif/.local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size =  16\n",
      "Learning_rate =  5e-05\n",
      "epochs =  8\n",
      "warmup percentage 0.1\n",
      "======== Epoch 1 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 2 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:56.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:07.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 3 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:07.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 4 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:07.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 5 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 6 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n",
      "  Batch   120  of    472.    Elapsed: 0:00:33.\n",
      "  Batch   160  of    472.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    472.    Elapsed: 0:00:55.\n",
      "  Batch   240  of    472.    Elapsed: 0:01:06.\n",
      "  Batch   280  of    472.    Elapsed: 0:01:18.\n",
      "  Batch   320  of    472.    Elapsed: 0:01:29.\n",
      "  Batch   360  of    472.    Elapsed: 0:01:40.\n",
      "  Batch   400  of    472.    Elapsed: 0:01:51.\n",
      "  Batch   440  of    472.    Elapsed: 0:02:02.\n",
      "======== Epoch 7 / 8 ========\n",
      "  Batch    40  of    472.    Elapsed: 0:00:11.\n",
      "  Batch    80  of    472.    Elapsed: 0:00:22.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id,function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "k8mT0UexeQec"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Call the garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Ensure CUDA is aware of the freed memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jTLkz2VCtjUX",
    "outputId": "906e4e7e-e581-4a77-bd9d-3b0a0df15198"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:32].unsqueeze(0).view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXncoQqIv3Ti"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02b9a6a75490438da5548ce15a0e3d8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d02facb2d6405da3d853450339ebb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae0b3835321b46e0b39fedc3fcab3cce",
      "placeholder": "​",
      "style": "IPY_MODEL_4b095ae8c9504e2ab2285802ed35ed91",
      "value": " 570/570 [00:00&lt;00:00, 7.92kB/s]"
     }
    },
    "0538292773c540f39b5af23e0615b063": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17f7e00bc9e745ea9561b07e7e926acf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19632de672904bb19d53996c5f61a51f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02b9a6a75490438da5548ce15a0e3d8a",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab1eae8c2a3347cc8494766fba11118c",
      "value": 231508
     }
    },
    "1a42d73d30bd4f6880c46526fc3ede83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7dcb99d856b94b068f4efcf783276c4e",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_812d8e4ff4384d64a55ce81f98588bb1",
      "value": 466062
     }
    },
    "222d862bfe6b4c86a4407447526ee2b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_483f44483ea24b86af2f619c5c0f065e",
       "IPY_MODEL_fb8ed742afeb468a86e96c733957b08f"
      ],
      "layout": "IPY_MODEL_7bcac0265f33462ba8454ea8f38b0ca5"
     }
    },
    "2c140a9be8b14ffd8f6ee533b02a7ae9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33716718d9cb426d93dc03c5c00bdd2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "34ecc7c661cc4fa2b3228137458fd9ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a9482003ee541b291216dc4b4cc6dc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3db025d6b1284221a8d5e6aba5cb7ffd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "418e255647c04016ba56a24bc35e5217": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c140a9be8b14ffd8f6ee533b02a7ae9",
      "placeholder": "​",
      "style": "IPY_MODEL_3a9482003ee541b291216dc4b4cc6dc8",
      "value": "tokenizer.json: 100%"
     }
    },
    "483f44483ea24b86af2f619c5c0f065e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5432be1e78f243a0b08c423cc845f31a",
      "placeholder": "​",
      "style": "IPY_MODEL_e65fc40dcef74ed9be651ec3c82fa933",
      "value": "0.012 MB of 0.012 MB uploaded\r"
     }
    },
    "496d7105b9944fe3bd6fec38082656a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49ab33ebbc8e4401a34d6b616846eada": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b095ae8c9504e2ab2285802ed35ed91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5432be1e78f243a0b08c423cc845f31a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "545a8c5b3288488aa43ed169a7124539": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f485543b68e44da96dd5d6a8b087d47",
       "IPY_MODEL_19632de672904bb19d53996c5f61a51f",
       "IPY_MODEL_78442e12e51a4c6288907cb9342347de"
      ],
      "layout": "IPY_MODEL_909c888073b3492b9c37702b274b9eb4"
     }
    },
    "579e015f20024190861e36d8bae9bdcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf18fec3a52c46d0869097f52a0c002c",
      "placeholder": "​",
      "style": "IPY_MODEL_e7fb6083b8464287a536d8a4032d3cd8",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "59076efdcdac4ef484d8df7a2171cc22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3db025d6b1284221a8d5e6aba5cb7ffd",
      "placeholder": "​",
      "style": "IPY_MODEL_34ecc7c661cc4fa2b3228137458fd9ba",
      "value": "config.json: 100%"
     }
    },
    "5f485543b68e44da96dd5d6a8b087d47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7d514670f0547d3a3327b43b04e8e09",
      "placeholder": "​",
      "style": "IPY_MODEL_c33fff6421db4235a17f70b22a37abd8",
      "value": "vocab.txt: 100%"
     }
    },
    "694a40dbace64ca6b608b3ecb85042f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_59076efdcdac4ef484d8df7a2171cc22",
       "IPY_MODEL_737dd8afd60a4284ad9310d09ea37809",
       "IPY_MODEL_04d02facb2d6405da3d853450339ebb2"
      ],
      "layout": "IPY_MODEL_912bd3c591f74305a49c1a4247155c6d"
     }
    },
    "737dd8afd60a4284ad9310d09ea37809": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73870ae9ed204cf8bd468921ac353abc",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c13e9516153e44db8395be037a3d54d8",
      "value": 570
     }
    },
    "73870ae9ed204cf8bd468921ac353abc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78442e12e51a4c6288907cb9342347de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f64122c8ce5248859dfdf340c34f917f",
      "placeholder": "​",
      "style": "IPY_MODEL_b9902f7d9a0d4cdbbca93d75ff1890c2",
      "value": " 232k/232k [00:00&lt;00:00, 472kB/s]"
     }
    },
    "7bcac0265f33462ba8454ea8f38b0ca5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dcb99d856b94b068f4efcf783276c4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "812d8e4ff4384d64a55ce81f98588bb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "86e14566cc3f4d59b949c4e1219a8206": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b02f21d8bbc4c669c17ec2299cc8255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f581039bd7848d1b892303f938ebfad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6a1ee9f3f57440dbb051f12ead5f662",
      "placeholder": "​",
      "style": "IPY_MODEL_8b02f21d8bbc4c669c17ec2299cc8255",
      "value": " 466k/466k [00:00&lt;00:00, 625kB/s]"
     }
    },
    "909c888073b3492b9c37702b274b9eb4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "912bd3c591f74305a49c1a4247155c6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3d741e6a0a94eaa90a5b4c4042ca349": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab1eae8c2a3347cc8494766fba11118c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae0b3835321b46e0b39fedc3fcab3cce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9902f7d9a0d4cdbbca93d75ff1890c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c13e9516153e44db8395be037a3d54d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c33fff6421db4235a17f70b22a37abd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7d514670f0547d3a3327b43b04e8e09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c89ded56995f48b4a074422989f46ce2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_579e015f20024190861e36d8bae9bdcb",
       "IPY_MODEL_e7899a82b3134f84ba1f3e0c45ddb1f5",
       "IPY_MODEL_ccc160fb75d54b7cb727cf5b25d847d6"
      ],
      "layout": "IPY_MODEL_a3d741e6a0a94eaa90a5b4c4042ca349"
     }
    },
    "ccc160fb75d54b7cb727cf5b25d847d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0538292773c540f39b5af23e0615b063",
      "placeholder": "​",
      "style": "IPY_MODEL_49ab33ebbc8e4401a34d6b616846eada",
      "value": " 48.0/48.0 [00:00&lt;00:00, 730B/s]"
     }
    },
    "cf18fec3a52c46d0869097f52a0c002c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d911e2cb6c964314a58bdfee2562276d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_418e255647c04016ba56a24bc35e5217",
       "IPY_MODEL_1a42d73d30bd4f6880c46526fc3ede83",
       "IPY_MODEL_8f581039bd7848d1b892303f938ebfad"
      ],
      "layout": "IPY_MODEL_17f7e00bc9e745ea9561b07e7e926acf"
     }
    },
    "e65fc40dcef74ed9be651ec3c82fa933": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e7899a82b3134f84ba1f3e0c45ddb1f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86e14566cc3f4d59b949c4e1219a8206",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_33716718d9cb426d93dc03c5c00bdd2f",
      "value": 48
     }
    },
    "e7fb6083b8464287a536d8a4032d3cd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebb15b86889a49b6965b2a697dc5246b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f64122c8ce5248859dfdf340c34f917f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6a1ee9f3f57440dbb051f12ead5f662": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb8ed742afeb468a86e96c733957b08f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_496d7105b9944fe3bd6fec38082656a7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ebb15b86889a49b6965b2a697dc5246b",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
